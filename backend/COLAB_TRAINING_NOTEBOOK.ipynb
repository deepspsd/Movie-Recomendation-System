{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommendation System - Model Training\n",
    "## Train Collaborative Filtering Models in Kaggle\n",
    "\n",
    "This notebook trains your ML models using the MovieLens 25M dataset.\n",
    "\n",
    "**Steps:**\n",
    "1. Add MovieLens dataset from Kaggle\n",
    "2. Install dependencies\n",
    "3. Train models (SVD, KNN, ALS)\n",
    "4. Download trained model file\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT FOR KAGGLE:**\n",
    "- Turn ON Internet in Settings (right sidebar)\n",
    "- Add MovieLens dataset: Click \"+ Add Data\" ‚Üí Search \"movielens-25m\" ‚Üí Add\n",
    "- Or we'll download it directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Download MovieLens 25M Dataset\n",
    "# For Kaggle: Check if dataset exists, if not download it\n",
    "\n",
    "import os\n",
    "\n",
    "# Check if running on Kaggle\n",
    "if os.path.exists('/kaggle/input'):\n",
    "    print(\"‚úÖ Running on Kaggle!\")\n",
    "    # Check for MovieLens dataset\n",
    "    if os.path.exists('/kaggle/input/movielens-25m-dataset'):\n",
    "        print(\"‚úÖ MovieLens dataset found in Kaggle input!\")\n",
    "        data_path = '/kaggle/input/movielens-25m-dataset'\n",
    "    else:\n",
    "        print(\"üì• Downloading MovieLens dataset...\")\n",
    "        !wget -q https://files.grouplens.org/datasets/movielens/ml-25m.zip\n",
    "        !unzip -q ml-25m.zip\n",
    "        data_path = 'ml-25m'\n",
    "        print(\"‚úÖ Dataset downloaded!\")\n",
    "else:\n",
    "    # Download if not on Kaggle\n",
    "    print(\"üì• Downloading MovieLens 25M dataset...\")\n",
    "    !wget -q https://files.grouplens.org/datasets/movielens/ml-25m.zip\n",
    "    !unzip -q ml-25m.zip\n",
    "    data_path = 'ml-25m'\n",
    "    print(\"‚úÖ Dataset downloaded!\")\n",
    "\n",
    "print(f\"\\nDataset location: {data_path}\")\n",
    "!ls {data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2.5: Monitor GPU Usage\n",
    "print(\"üîç Checking GPU status...\")\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU is available!\")\n",
    "    print(f\"   Device count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"   Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"‚úÖ GPU cache cleared and ready!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected - will use CPU\")\n",
    "    print(\"   Training will be slower but still work\")\n",
    "\n",
    "print(\"\\nüí° TIP: You should see GPU usage increase during training!\")\n",
    "print(\"   Watch the GPU percentage in the session stats (right sidebar)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Install Required Libraries (GPU-Accelerated)\n",
    "print(\"üì¶ Installing GPU-accelerated libraries...\")\n",
    "!pip install -q pandas numpy scikit-learn scipy torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import csr_matrix\n",
    "import pickle\n",
    "import logging\n",
    "import json\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU count: {gpu_count}\")\n",
    "    for i in range(gpu_count):\n",
    "        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    \n",
    "    # Test GPU\n",
    "    test_tensor = torch.rand(5, 5).to(device)\n",
    "    print(f\"‚úÖ GPU test successful! Tensor device: {test_tensor.device}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected, using CPU\")\n",
    "\n",
    "print(\"‚úÖ Libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Define MEMORY-OPTIMIZED Collaborative Filtering Model\n",
    "class CollaborativeFilteringModel:\n",
    "    def __init__(self, use_gpu=True):\n",
    "        self.user_movie_matrix = None\n",
    "        self.user_similarity_matrix = None\n",
    "        self.user_ids = None\n",
    "        self.movie_ids = None\n",
    "        self.svd_model = None\n",
    "        self.knn_model = None\n",
    "        self.user_factors = None\n",
    "        self.item_factors = None\n",
    "        self.n_factors = 50\n",
    "        \n",
    "        # GPU setup\n",
    "        self.use_gpu = use_gpu and torch.cuda.is_available()\n",
    "        self.device = torch.device('cuda' if self.use_gpu else 'cpu')\n",
    "        print(f\"üîß Model initialized on: {self.device}\")\n",
    "        \n",
    "    def prepare_data(self, ratings_data, movies_data):\n",
    "        try:\n",
    "            print(\"   Converting to DataFrames...\")\n",
    "            self.ratings_df = pd.DataFrame(ratings_data)\n",
    "            self.movies_df = pd.DataFrame(movies_data)\n",
    "            \n",
    "            print(\"   Creating user-movie matrix (sparse)...\")\n",
    "            # Use sparse matrix to save memory\n",
    "            self.user_movie_matrix = self.ratings_df.pivot_table(\n",
    "                index='user_id', columns='movie_id', values='rating'\n",
    "            ).fillna(0)\n",
    "            \n",
    "            # Limit matrix size if too large\n",
    "            if self.user_movie_matrix.shape[0] > 5000:\n",
    "                print(f\"   ‚ö†Ô∏è Large matrix detected, sampling users...\")\n",
    "                sampled_users = np.random.choice(self.user_movie_matrix.index, 5000, replace=False)\n",
    "                self.user_movie_matrix = self.user_movie_matrix.loc[sampled_users]\n",
    "            \n",
    "            self.user_ids = list(self.user_movie_matrix.index)\n",
    "            self.movie_ids = list(self.user_movie_matrix.columns)\n",
    "            print(f\"‚úÖ Data prepared: {len(self.user_ids)} users, {len(self.movie_ids)} movies\")\n",
    "            \n",
    "            # Clean up\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def compute_user_similarity(self):\n",
    "        \"\"\"Compute similarity with memory optimization\"\"\"\n",
    "        print(\"   Computing similarity matrix (memory-optimized)...\")\n",
    "        try:\n",
    "            if self.use_gpu:\n",
    "                # Process in batches to avoid GPU OOM\n",
    "                batch_size = 500\n",
    "                n_users = len(self.user_ids)\n",
    "                \n",
    "                print(f\"   Processing {n_users} users in batches of {batch_size}...\")\n",
    "                similarity_matrix = np.zeros((n_users, n_users), dtype=np.float32)\n",
    "                \n",
    "                matrix_np = self.user_movie_matrix.values.astype(np.float32)\n",
    "                \n",
    "                for i in range(0, n_users, batch_size):\n",
    "                    end_i = min(i + batch_size, n_users)\n",
    "                    batch = torch.tensor(matrix_np[i:end_i], device=self.device, dtype=torch.float32)\n",
    "                    full_matrix = torch.tensor(matrix_np, device=self.device, dtype=torch.float32)\n",
    "                    \n",
    "                    # Normalize\n",
    "                    batch_norm = torch.nn.functional.normalize(batch, p=2, dim=1)\n",
    "                    full_norm = torch.nn.functional.normalize(full_matrix, p=2, dim=1)\n",
    "                    \n",
    "                    # Compute similarity for this batch\n",
    "                    similarity_batch = torch.mm(batch_norm, full_norm.t())\n",
    "                    similarity_matrix[i:end_i] = similarity_batch.cpu().numpy()\n",
    "                    \n",
    "                    # Clear GPU memory\n",
    "                    del batch, full_matrix, batch_norm, full_norm, similarity_batch\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "                    if (i // batch_size + 1) % 5 == 0:\n",
    "                        print(f\"   Progress: {end_i}/{n_users} users processed\")\n",
    "                \n",
    "                self.user_similarity_matrix = similarity_matrix\n",
    "                print(\"   ‚úÖ Computed on GPU (batched)\")\n",
    "            else:\n",
    "                # CPU with memory optimization\n",
    "                from sklearn.metrics.pairwise import cosine_similarity\n",
    "                self.user_similarity_matrix = cosine_similarity(self.user_movie_matrix.values.astype(np.float32))\n",
    "                print(\"   ‚úÖ Computed on CPU\")\n",
    "            \n",
    "            print(\"‚úÖ User similarity computed\")\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in similarity computation: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def train_svd_model(self, n_components=30):\n",
    "        \"\"\"Reduced components for memory efficiency\"\"\"\n",
    "        print(f\"   Fitting SVD model ({n_components} components)...\")\n",
    "        sparse_matrix = csr_matrix(self.user_movie_matrix.values.astype(np.float32))\n",
    "        self.svd_model = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "        self.svd_model.fit(sparse_matrix)\n",
    "        print(f\"‚úÖ SVD trained ({n_components} components)\")\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        return True\n",
    "    \n",
    "    def train_knn_model(self, n_neighbors=15):\n",
    "        \"\"\"Reduced neighbors for faster training\"\"\"\n",
    "        print(f\"   Fitting KNN model ({n_neighbors} neighbors)...\")\n",
    "        self.knn_model = NearestNeighbors(\n",
    "            n_neighbors=n_neighbors, \n",
    "            metric='cosine', \n",
    "            algorithm='brute',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        self.knn_model.fit(self.user_movie_matrix.values.astype(np.float32))\n",
    "        print(f\"‚úÖ KNN trained ({n_neighbors} neighbors)\")\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        return True\n",
    "    \n",
    "    def train_als_model(self, n_factors=30, n_iterations=5, lambda_reg=0.1):\n",
    "        \"\"\"OPTIMIZED ALS: Fewer factors and iterations\"\"\"\n",
    "        print(f\"   Training ALS model ({n_factors} factors, {n_iterations} iterations)...\")\n",
    "        self.n_factors = n_factors\n",
    "        R = csr_matrix(self.user_movie_matrix.values.astype(np.float32))\n",
    "        n_users, n_items = R.shape\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            print(\"   üöÄ Using GPU for ALS training\")\n",
    "            # Convert to dense only for small matrices\n",
    "            if n_users * n_items < 10_000_000:  # ~10M elements\n",
    "                R_dense = torch.tensor(R.toarray(), device=self.device, dtype=torch.float32)\n",
    "                \n",
    "                # Initialize factors on GPU\n",
    "                torch.manual_seed(42)\n",
    "                user_factors = torch.randn(n_users, n_factors, device=self.device, dtype=torch.float32) * 0.1\n",
    "                item_factors = torch.randn(n_items, n_factors, device=self.device, dtype=torch.float32) * 0.1\n",
    "                \n",
    "                # Training loop\n",
    "                for iteration in range(n_iterations):\n",
    "                    user_factors = self._als_step_gpu(R_dense, item_factors, lambda_reg)\n",
    "                    item_factors = self._als_step_gpu(R_dense.t(), user_factors, lambda_reg)\n",
    "                    \n",
    "                    # Compute RMSE every iteration\n",
    "                    predictions = torch.mm(user_factors, item_factors.t())\n",
    "                    mask = R_dense > 0\n",
    "                    diff = (R_dense[mask] - predictions[mask]) ** 2\n",
    "                    rmse = torch.sqrt(diff.mean()).item()\n",
    "                    print(f\"  Iteration {iteration+1}/{n_iterations}, RMSE: {rmse:.4f}\")\n",
    "                    \n",
    "                    # Clear cache\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                self.user_factors = user_factors.cpu().numpy()\n",
    "                self.item_factors = item_factors.cpu().numpy()\n",
    "                del R_dense, user_factors, item_factors\n",
    "            else:\n",
    "                print(\"   Matrix too large for GPU, using CPU...\")\n",
    "                self._train_als_cpu(R, n_factors, n_iterations, lambda_reg)\n",
    "        else:\n",
    "            self._train_als_cpu(R, n_factors, n_iterations, lambda_reg)\n",
    "        \n",
    "        torch.cuda.empty_cache() if self.use_gpu else None\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        print(\"‚úÖ ALS trained\")\n",
    "        return True\n",
    "    \n",
    "    def _train_als_cpu(self, R, n_factors, n_iterations, lambda_reg):\n",
    "        \"\"\"CPU ALS training\"\"\"\n",
    "        print(\"   Using CPU for ALS training\")\n",
    "        n_users, n_items = R.shape\n",
    "        np.random.seed(42)\n",
    "        self.user_factors = np.random.normal(0, 0.1, (n_users, n_factors)).astype(np.float32)\n",
    "        self.item_factors = np.random.normal(0, 0.1, (n_items, n_factors)).astype(np.float32)\n",
    "        \n",
    "        for iteration in range(n_iterations):\n",
    "            self.user_factors = self._als_step_cpu(R, self.item_factors, lambda_reg)\n",
    "            self.item_factors = self._als_step_cpu(R.T, self.user_factors, lambda_reg)\n",
    "            \n",
    "            # Compute RMSE\n",
    "            predictions = self.user_factors @ self.item_factors.T\n",
    "            mask = R.toarray() > 0\n",
    "            rmse = np.sqrt(np.mean((R.toarray()[mask] - predictions[mask]) ** 2))\n",
    "            print(f\"  Iteration {iteration+1}/{n_iterations}, RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    def _als_step_gpu(self, R, fixed_factors, lambda_reg):\n",
    "        \"\"\"GPU ALS step with memory optimization\"\"\"\n",
    "        n_users, n_factors = R.shape[0], fixed_factors.shape[1]\n",
    "        updated_factors = torch.zeros(n_users, n_factors, device=self.device, dtype=torch.float32)\n",
    "        lambda_eye = lambda_reg * torch.eye(n_factors, device=self.device, dtype=torch.float32)\n",
    "        \n",
    "        # Process in batches\n",
    "        batch_size = 100\n",
    "        for start in range(0, n_users, batch_size):\n",
    "            end = min(start + batch_size, n_users)\n",
    "            for u in range(start, end):\n",
    "                rated_mask = R[u] > 0\n",
    "                if not rated_mask.any():\n",
    "                    continue\n",
    "                \n",
    "                ratings = R[u][rated_mask]\n",
    "                factors = fixed_factors[rated_mask]\n",
    "                \n",
    "                A = torch.mm(factors.t(), factors) + lambda_eye\n",
    "                b = torch.mv(factors.t(), ratings)\n",
    "                \n",
    "                try:\n",
    "                    updated_factors[u] = torch.linalg.solve(A, b)\n",
    "                except:\n",
    "                    updated_factors[u] = torch.linalg.lstsq(A.unsqueeze(0), b.unsqueeze(0)).solution.squeeze()\n",
    "        \n",
    "        return updated_factors\n",
    "    \n",
    "    def _als_step_cpu(self, R, fixed_factors, lambda_reg):\n",
    "        \"\"\"CPU ALS step\"\"\"\n",
    "        n_users, n_factors = R.shape[0], fixed_factors.shape[1]\n",
    "        updated_factors = np.zeros((n_users, n_factors), dtype=np.float32)\n",
    "        \n",
    "        for u in range(n_users):\n",
    "            rated_items = R[u].nonzero()[1]\n",
    "            if len(rated_items) == 0:\n",
    "                continue\n",
    "            \n",
    "            ratings = R[u, rated_items].toarray().flatten().astype(np.float32)\n",
    "            factors = fixed_factors[rated_items].astype(np.float32)\n",
    "            \n",
    "            A = factors.T @ factors + lambda_reg * np.eye(n_factors, dtype=np.float32)\n",
    "            b = factors.T @ ratings\n",
    "            \n",
    "            try:\n",
    "                updated_factors[u] = np.linalg.solve(A, b)\n",
    "            except:\n",
    "                updated_factors[u] = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "        \n",
    "        return updated_factors\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        model_data = {\n",
    "            'user_movie_matrix': self.user_movie_matrix,\n",
    "            'user_similarity_matrix': self.user_similarity_matrix,\n",
    "            'user_ids': self.user_ids,\n",
    "            'movie_ids': self.movie_ids,\n",
    "            'svd_model': self.svd_model,\n",
    "            'knn_model': self.knn_model,\n",
    "            'user_factors': self.user_factors,\n",
    "            'item_factors': self.item_factors,\n",
    "            'n_factors': self.n_factors\n",
    "        }\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\"‚úÖ Model saved to {filepath}\")\n",
    "        return True\n",
    "\n",
    "print(\"‚úÖ Memory-optimized model class loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Load and Sample Data (OPTIMIZED FOR KAGGLE)\n",
    "print(\"üìÇ Loading MovieLens data...\")\n",
    "\n",
    "# Use the data_path from Step 1\n",
    "movies_df = pd.read_csv(f'{data_path}/movies.csv')\n",
    "print(f\"‚úÖ Loaded {len(movies_df):,} movies\")\n",
    "\n",
    "# OPTIMIZED: Load ratings in chunks to avoid memory issues\n",
    "print(f\"üì• Loading ratings (this may take a moment)...\")\n",
    "ratings_df = pd.read_csv(f'{data_path}/ratings.csv', \n",
    "                         usecols=['userId', 'movieId', 'rating'],  # Only load needed columns\n",
    "                         dtype={'userId': 'int32', 'movieId': 'int32', 'rating': 'float32'})  # Use smaller dtypes\n",
    "print(f\"‚úÖ Loaded {len(ratings_df):,} ratings\")\n",
    "\n",
    "# KAGGLE-OPTIMIZED SAMPLE SIZES (prevents kernel crashes)\n",
    "SAMPLE_SIZE = 50000  # REDUCED from 100k - safer for Kaggle\n",
    "# Recommended sizes:\n",
    "# - 50k ratings = 5-10 min, LOW memory usage ‚úÖ SAFEST\n",
    "# - 100k ratings = 10-15 min, MEDIUM memory usage\n",
    "# - 200k ratings = 20-30 min, HIGH memory usage (risky on Kaggle)\n",
    "\n",
    "print(f\"\\nüìä Sampling {SAMPLE_SIZE:,} ratings for training...\")\n",
    "print(\"üí° Using smaller sample to prevent kernel crashes\")\n",
    "\n",
    "# Smart sampling: Get diverse users and movies\n",
    "ratings_sample = ratings_df.sample(n=min(SAMPLE_SIZE, len(ratings_df)), random_state=42)\n",
    "\n",
    "# Get only movies that appear in sample\n",
    "movie_ids_in_sample = ratings_sample['movieId'].unique()\n",
    "movies_sample = movies_df[movies_df['movieId'].isin(movie_ids_in_sample)]\n",
    "\n",
    "print(f\"‚úÖ Sampled: {len(movies_sample):,} movies, {len(ratings_sample):,} ratings\")\n",
    "print(f\"üìä Memory usage: ~{ratings_sample.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Clear original dataframes to free memory\n",
    "del ratings_df\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"‚úÖ Memory cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Format Data\n",
    "print(\"üîÑ Formatting data...\")\n",
    "ratings_data = [{\n",
    "    'user_id': f\"user_{row['userId']}\",\n",
    "    'movie_id': int(row['movieId']),\n",
    "    'rating': float(row['rating'])\n",
    "} for _, row in ratings_sample.iterrows()]\n",
    "\n",
    "movies_data = [{\n",
    "    'id': int(row['movieId']),\n",
    "    'title': row['title'],\n",
    "    'genres': json.dumps([{\"name\": g} for g in row['genres'].split('|')])\n",
    "} for _, row in movies_sample.iterrows()]\n",
    "\n",
    "print(f\"‚úÖ Formatted {len(ratings_data):,} ratings, {len(movies_data):,} movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: Train All Models (MEMORY-OPTIMIZED)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ü§ñ TRAINING MODELS (MEMORY-OPTIMIZED)\")\n",
    "print(\"=\"*60)\n",
    "print(\"üí° Using reduced parameters to prevent kernel crashes\")\n",
    "\n",
    "model = CollaborativeFilteringModel(use_gpu=True)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ Preparing data...\")\n",
    "model.prepare_data(ratings_data, movies_data)\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Computing user similarity (batched)...\")\n",
    "model.compute_user_similarity()\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ Training SVD (30 components)...\")\n",
    "model.train_svd_model(n_components=30)  # Reduced from 50\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ Training KNN (15 neighbors)...\")\n",
    "model.train_knn_model(n_neighbors=15)  # Reduced from 20\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£ Training ALS (30 factors, 5 iterations)...\")\n",
    "model.train_als_model(n_factors=30, n_iterations=5)  # Reduced from 50 factors, 10 iterations\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ ALL MODELS TRAINED!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nüìä GPU Memory Usage:\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**2\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**2\n",
    "        print(f\"   GPU {i}: {allocated:.1f} MB allocated, {reserved:.1f} MB reserved\")\n",
    "    \n",
    "    # Final cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"‚úÖ GPU cache cleared\")\n",
    "\n",
    "# CPU memory cleanup\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"‚úÖ Memory optimized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: Save Model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üíæ SAVING MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_filename = 'collaborative_filtering_trained.pkl'\n",
    "print(f\"Saving model to: {model_filename}\")\n",
    "\n",
    "if model.save_model(model_filename):\n",
    "    print(f\"‚úÖ Model saved successfully!\")\n",
    "    \n",
    "    # Get file size\n",
    "    import os\n",
    "    file_size = os.path.getsize(model_filename) / (1024 * 1024)\n",
    "    print(f\"üì¶ File size: {file_size:.1f} MB\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to save model\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 8: Download Model (Kaggle & Colab Compatible)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üì• PREPARING MODEL FOR DOWNLOAD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check environment\n",
    "if os.path.exists('/kaggle/working'):\n",
    "    print(\"‚úÖ Running on Kaggle!\")\n",
    "    print(f\"‚úÖ Model saved to: /kaggle/working/{model_filename}\")\n",
    "    print(\"\\nüì• TO DOWNLOAD:\")\n",
    "    print(\"1. Click 'Save Version' (top right, blue button)\")\n",
    "    print(\"2. Select 'Save & Run All (Quick Save)'\")\n",
    "    print(\"3. Wait for completion notification\")\n",
    "    print(\"4. Go to 'Output' tab (right sidebar)\")\n",
    "    print(\"5. Click download icon next to the .pkl file\")\n",
    "    print(\"\\nüí° The file will be in the Output section after saving!\")\n",
    "    \n",
    "elif os.path.exists('/content'):\n",
    "    print(\"‚úÖ Running on Google Colab!\")\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        print(\"üì• Downloading model file...\")\n",
    "        files.download(model_filename)\n",
    "        print(\"‚úÖ Download started! Check your browser downloads.\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Auto-download failed. File saved as:\", model_filename)\n",
    "else:\n",
    "    print(\"‚úÖ Running locally!\")\n",
    "    print(f\"Model saved as: {model_filename}\")\n",
    "    print(\"Copy this file to: d:\\\\Movie recommendation system\\\\backend\\\\saved_models\\\\\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Next Steps\n",
    "\n",
    "### **For Kaggle Users:**\n",
    "\n",
    "After the notebook finishes:\n",
    "\n",
    "1. **Click \"Save Version\"** (top right, blue button)\n",
    "2. **Select \"Save & Run All\"** (Quick Save)\n",
    "3. **Wait for it to complete** (check notification)\n",
    "4. **Go to the \"Output\" tab** (right side)\n",
    "5. **Download** `collaborative_filtering_trained.pkl`\n",
    "\n",
    "### **For Colab Users:**\n",
    "\n",
    "The file downloads automatically to your browser's download folder.\n",
    "\n",
    "---\n",
    "\n",
    "### **Place the Downloaded File:**\n",
    "\n",
    "```\n",
    "d:\\Movie recommendation system\\backend\\saved_models\\collaborative_filtering_trained.pkl\n",
    "```\n",
    "\n",
    "Create the `saved_models` folder if it doesn't exist.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Kaggle Tips\n",
    "\n",
    "- **Turn ON Internet:** Settings ‚Üí Internet ‚Üí ON (required for pip installs)\n",
    "- **Use GPU:** Settings ‚Üí Accelerator ‚Üí GPU (faster training)\n",
    "- **Increase Sample Size:** Kaggle gives 30 hours/week, so you can use `SAMPLE_SIZE = 500000`\n",
    "- **Save Often:** Click \"Save Version\" to avoid losing progress\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Training Options\n",
    "\n",
    "- **Quick (5-10 min):** `SAMPLE_SIZE = 50000`\n",
    "- **Medium (10-20 min):** `SAMPLE_SIZE = 100000`\n",
    "- **Better (20-40 min):** `SAMPLE_SIZE = 200000`\n",
    "- **Best (1-2 hours):** `SAMPLE_SIZE = 1000000` or remove sampling\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Training! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
